# -*- coding: utf-8 -*-
"""Nur Aula_Project 5

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xQMImFO8pP8OPT86n5kK6_F1z3P-88uL

# **1. Importing Libraries**
"""

# Install required packages
!pip install scikit-plot
!pip install kmodes

# Data manipulation and analysis
import pandas as pd
import numpy as np

# Data visualization
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

# Machine Learning Clustering
from sklearn.cluster import KMeans
from kmodes.kprototypes import KPrototypes
from yellowbrick.cluster import KElbowVisualizer

# Scaling numeric features
from sklearn.preprocessing import StandardScaler

# PCA
from sklearn.decomposition import PCA

# Cluster Validation
from sklearn.metrics import silhouette_score, adjusted_rand_score

# Resample data
from sklearn.utils import resample

# Statistical analysis and hypothesis testing
from scipy import stats

# Interactive maps
import folium
from datetime import datetime

# Machine Learning plotting
import scikitplot as skplt

"""# **2. Exploratory Data Analysis (EDA) & Data Preprocessing**

## **2.1 Read Data Frames**
"""

url = "https://github.com/nuraulaola/ds-batch-32-project-5/raw/main/datasets/OnlineRetail.csv"
df = pd.read_csv(url, encoding='ISO-8859-1') # Read dataset
df.head()  # Show the first 5 rows of df

"""## **2.2 Understanding Each Columns**"""

print("List of data types for each column:\n", df.dtypes) # Check the data types of each column

print("\nList of missing values for each column:\n", df.isnull().sum()) # Check missing values

df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], errors='coerce') # Convert data type to datetime
print("\nInvoiceDate data type after adjustment:\n", df['InvoiceDate'].dtypes) # Check if the data type is now datetime

print("\nList of UnitPrice or Quantity with negative or zero values:\n", df[(df['UnitPrice'] <= 0) | (df['Quantity'] <= 0)]) # Check price and quantity

print("\nList of empty or invalid CustomerID:\n", df[df['CustomerID'].isnull()]) # Check customer IDs

print("\nList of empty item descriptions:\n", df[df['Description'].isnull()]) # Check item descriptions

print("\nList of unique values in the Country column:\n", df['Country'].unique()) # Check the Country column

duplicated_invoice_nos = df[df.duplicated(subset=['InvoiceNo'], keep=False)] # Check rows with duplicated 'InvoiceNo'
print("\nRows with duplicated 'InvoiceNo':\n", duplicated_invoice_nos)

duplicated_stock_codes = df[df.duplicated(subset=['StockCode'], keep=False)] # Check rows with duplicated 'StockCode'
print("\nRows with duplicated 'StockCode':\n", duplicated_stock_codes)

duplicated_customer_ids = df[df.duplicated(subset=['CustomerID'], keep=False)] # Check rows with duplicated 'CustomerID'
print("\nRows with duplicated 'CustomerID':\n", duplicated_customer_ids)

"""## **2.3 Handling Columns**"""

df_cleaned = df.drop(df[(df['UnitPrice'] <= 0) | (df['Quantity'] <= 0)].index) # Remove rows with negative or zero values for price and quantity
print("Data Frames after removing rows with negative or zero values for price and quantity\n", df_cleaned)
df_without_missing_customer_id = df.dropna(subset=['CustomerID']) # Try examining Data Frames without rows where the 'CustomerID' is missing
print("\nAnalysis results with all data:\n", df)
print("\nAnalysis results without rows with missing 'CustomerID':\n", df_without_missing_customer_id)
df['Description'] = df_cleaned['Description'].fillna('Unknown') # Fill empty descriptions with 'Unknown'
print("DataFrame after handling missing values in the Description column:\n", df)
# Possible scenarios causing duplicate 'CustomerID' and 'InvoiceNo': there might be the same transaction at different times
grouped_df = df_cleaned.groupby(['CustomerID', 'InvoiceNo', 'InvoiceDate']).agg({
    'StockCode': 'first',
    'Description': 'first',
    'Quantity': 'sum',
    'UnitPrice': 'first',
    'Country': 'first'
}).reset_index() # Try handling duplicates by aggregating data based on 'CustomerID', 'InvoiceNo', and 'InvoiceDate'
print("DataFrame after handling duplicates:\n", grouped_df)

"""## **2.4 In-Depth EDA**

### **2.4.1 Top 5 Customers with the Highest Purchases**
"""

customer_data = grouped_df.copy()
total_purchases = customer_data.groupby('CustomerID')['Quantity'].sum().reset_index() # Total purchases from each cust
sorted_customers = total_purchases.sort_values(by='Quantity', ascending=False) # Sorting from the highest purchases
top_five_customers = sorted_customers.head(5) # Show only 5 custs with the highest purchases
plt.figure(figsize=(10, 6))
ax = sns.barplot(x='CustomerID', y='Quantity', data=top_five_customers, order=top_five_customers['CustomerID'], palette='RdPu')
for p in ax.patches:
    ax.annotate(f'{p.get_height():.0f}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', xytext=(0, 10), textcoords='offset points')
plt.xlabel('CustomerID')
plt.ylabel('Total Purchases (Quantity)')
plt.title('Top 5 Customers with the Highest Purchases')
plt.tight_layout()  # Adjust layout
plt.show()

"""Conclusion 🔍

The customers with the highest purchases are as follows:

1. Customer 14646.0 with the highest purchase of 196915 units,

2. Customer 16446.0 with the highest purchase of 80997 units,

3. Customer 14911.0 with the highest purchase of 80265 units,

4. Customer 12415.0 with the highest purchase of 77374 units,

5. Customer 12346.0 with the highest purchase of 74215 units.

### **2.4.2 Time with the Highest Number of Customers**
"""

date_unpacked_df = grouped_df.copy()
date_unpacked_df['Day'] = date_unpacked_df['InvoiceDate'].dt.day # Extract day from 'InvoiceDate'
date_unpacked_df['Hour'] = date_unpacked_df['InvoiceDate'].dt.hour # Extract hour from 'InvoiceDate'
date_unpacked_df['Month'] = date_unpacked_df['InvoiceDate'].dt.month # Extract month from 'InvoiceDate'
date_unpacked_df['Week'] = date_unpacked_df['InvoiceDate'].dt.isocalendar().week # Extract week from 'InvoiceDate'
time_with_highest_customers = date_unpacked_df.groupby(['Day', 'Hour', 'Month', 'Week'])['CustomerID'].nunique().idxmax() # Time with highest cust
day, hour, month, week = time_with_highest_customers # Unpack the tuple
print(f"So, the time with the highest number of customers occurs on the:\n {day}th day of the {month}th month " \
         f"({pd.to_datetime(1, format='%m').strftime('%B')}) at {hour:02d}:00 (noon) during the {week}th week of the year.")

"""### **2.4.3 Top-Selling Products**"""

top_selling_products = grouped_df.groupby('Description')['Quantity'].sum().sort_values(ascending=False).head(10).reset_index()
plt.figure(figsize=(10, 6))
ax = sns.barplot(x='Description', y='Quantity', data=top_selling_products, order=top_selling_products['Description'], palette='RdPu')
ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')  # Rotate x-axis labels
for p in ax.patches:
    ax.annotate(f'{p.get_height():.0f}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', xytext=(0, 10), textcoords='offset points')
plt.xlabel('Product Description')
plt.ylabel('Total Purchases (Quantity)')
plt.title('Top 10 Products with the Highest Purchases')
plt.tight_layout()  # Adjust layout
plt.show()

"""Conclusion 🔍

The top-selling products are as follows:

1. PAPER CRAFT , LITTLE BIRDIE with total purchases of 80995 units,

2. MEDIUM CERAMIC TOP STORAGE JAR with total purchases of 75959 units,

3. WHITE HANGING HEART T-LIGHT HOLDER with total purchases of 53433 units,

4. REGENCY CAKESTAND 3 TIER with total purchases of 39248 units,

5. RABBIT NIGHT LIGHT with total purchases of 38840 units,

6. SET OF 3 REGENCY CAKE TINS with total purchases of 31172 units,

7. PAPER CHAIN KIT 50'S CHRISTMAS  with total purchases of 31042 units,

8. WOOD BLACK BOARD ANT WHITE FINISH with total purchases of 30093 units,

9. RED  HARMONICA IN BOX  with total purchases of 28292 units,

10. WORLD WAR 2 GLIDERS ASSTD DESIGNS with total purchases of 25213 units.

### **2.4.4 Revenue Trend Over Time**
"""

grouped_df['Revenue'] = grouped_df['Quantity'] * grouped_df['UnitPrice']
revenue_trend = grouped_df.groupby('InvoiceDate')['Revenue'].sum().reset_index()
plt.figure(figsize=(14, 8))
sns.lineplot(data=revenue_trend, x='InvoiceDate', y='Revenue', color='pink')
plt.title('Revenue Trend Over Time')
plt.xlabel('Invoice Date')
plt.ylabel('Total Revenue')
plt.tight_layout()  # Adjust layout
plt.show()

"""Conclusion 🔍

1. The highest revenue point is in September 2011, at around $240,000.

2. Despite the fluctuations, the overall trend is for revenue to increase over time.

## **2.5 Data Preprocessing**

### **2.5.1 Separating Data Frames**
"""

numeric_columns = ['Quantity', 'UnitPrice', 'Revenue']  # Extract numeric columns
categorical_columns = ['CustomerID', 'InvoiceNo', 'StockCode', 'Description', 'Country'] # Extract categorical columns
numeric_data = grouped_df[numeric_columns] # Separate the DataFrame
categorical_data = grouped_df[categorical_columns]

"""### **2.5.2 Standardize Numeric Features**"""

scaler = StandardScaler()
numeric_data_scaled = scaler.fit_transform(numeric_data) # Standardize numeric data

"""### **2.5.3 Combine Scaled Number Features with Categorical Features**"""

data_combined = pd.concat([pd.DataFrame(numeric_data_scaled, columns=numeric_columns), categorical_data], axis=1) # Combine scaled numeric data with categorical data
cat_columns_indices = [data_combined.columns.get_loc(col) for col in categorical_columns] # Specify the indices of categorical columns

"""# **3. Clustering**

## **3.1 Cluster Formation**
"""

num_clusters = 3 # Clusters number
kproto = KPrototypes(n_clusters=num_clusters, init='Cao', n_init=2, verbose=2, random_state=123) # Fit k-Prototypes model
clusters = kproto.fit_predict(data_combined.values, categorical=cat_columns_indices)
grouped_df['Cluster'] = clusters # Add cluster labels
grouped_df.head()

"""Conclusion 🔍

1. On the 14th iteration, moves were already zero in the second run, while in the first run, moves only reached zero on the 17th iteration. This indicates that in the second run, the algorithm achieves convergence faster in grouping data points into clusters.

2. However, even though the second run achieves convergence faster in grouping data points, the evaluation of "best" is typically based on a lower ncost value. The first run exhibits a lower ncost, making it considered a better clustering result. This suggests that the assessment is more oriented towards clustering quality rather than convergence speed.

## **3.2 Cluster Validation**
"""

silhouette_kproto = silhouette_score(numeric_data_scaled, grouped_df['Cluster']) # Evaluate clustering using Silhouette Score
print(f"Silhouette Score for k-Prototypes: {silhouette_kproto}")

"""Insight ⭐

A Silhouette Score of 0.98 for 3 clusters is very high and close to the maximum value of 1. This suggests that the clusters are well-separated and distinct from each other. High Silhouette Scores generally indicate good clustering results, where each data point is closer to its own cluster than to the neighboring clusters.

## **3.2  Analysis of Cluster Characteristics**

### **3.2.1 Cluster Profiles Analysis**
"""

cluster_profiles = grouped_df.groupby('Cluster').mean() # Analyze cluster profiles
cluster_profiles.plot(kind='bar', figsize=(12, 6), colormap='PuRd') # Plot cluster profiles
plt.title('Cluster Profiles')
plt.xlabel('Cluster')
plt.ylabel('Mean Value')
plt.legend(title='Feature', bbox_to_anchor=(1, 1))
plt.tight_layout() # Auto adjust
plt.show()

"""Insights ⭐

1. **Cluster 0:** Customers in Cluster 0 have relatively lower average values across all metrics, indicating that they might be making smaller and less frequent purchases

2. **Cluster 1:** Customers in Cluster 1 have moderate average values for 'Quantity' and 'Revenue', suggesting they make moderate-sized purchases.

3. **Cluster 2:** Customers in Cluster 2 stand out with significantly higher average values for 'Quantity' and 'Revenue'. This cluster likely represents high-value customers who make large and frequent purchases.

### **3.2.2 Cluster Separation Visualization**
"""

pca = PCA(n_components=2) # Visualize cluster separation using PCA
data_pca = pca.fit_transform(numeric_data_scaled)
data_pca_df = pd.DataFrame(data_pca, columns=['PC1', 'PC2'])
data_pca_df['Cluster'] = clusters

plt.figure(figsize=(10, 6))
for cluster in range(num_clusters):
    cluster_data = data_pca_df[data_pca_df['Cluster'] == cluster]
    plt.scatter(cluster_data['PC1'], cluster_data['PC2'], label=f'Cluster {cluster}')

plt.title('PCA - Cluster Separation')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend()
plt.show()

"""Conclusion 🔍

1. Cluster 2 may have significantly different values in one or several features compared to other clusters. This can result in the separation of Cluster 2 in visualizations.

2. The relatively dense nature of Cluster 1 suggests that the data within this cluster has lower variability or exhibits a more uniform pattern in certain features.

### **3.2.3 Number of Observations in Each Cluster**
"""

clustered_groups = grouped_df.groupby('Cluster')
num_observations_summary = pd.DataFrame({'Num_Observations': clustered_groups.size()})

# Bar plot for number of observations in each cluster
plt.figure(figsize=(8, 5))
sns.barplot(x=num_observations_summary.index, y='Num_Observations', data=num_observations_summary, palette='PuRd')
plt.yscale('log') # Set logarithmic scale to better visualize clusters with smaller counts
plt.title('Number of Observations in Each Cluster')
plt.xlabel('Cluster')
plt.ylabel('Number of Observations (log scale)')
plt.tight_layout() # Auto adjust
plt.show()

"""Conclusion 🔍

1. **Cluster 0:** Contains 11 observations.

2. **Cluster 1:** Contains 18,544 observations.

3. **Cluster 2:** Contains 7 observations.

In summary, the "Num_Observations" column represents the count of data points within each cluster. It indicates how many data points are assigned to each cluster based on the clustering analysis. In this case, Cluster 1 has a significantly larger number of observations compared to Clusters 0 and 2.

## **3.3  Promotions for Each Cluster**
"""

def create_promotions(grouped_df): # To create promotions based on cluster
    if cluster == 0:
        return "Frequent Shopper Discounts, Bundle Deals"
    elif cluster == 1:
        return "Tiered Discounts, Limited-Time Offers"
    elif cluster == 2:
        return "Exclusive VIP Offers, Reward Programs"

grouped_df['Promotions'] = grouped_df['Cluster'].apply(create_promotions)
print(grouped_df[['Cluster', 'Promotions']])

"""# **4. Clustering for Non-UK Customers**

## **4.1 Prepare Non-UK Customers DataFrame**
"""

non_uk_df = grouped_df[grouped_df['Country'] != 'United Kingdom']  # Filtering condition
numeric_features = non_uk_df[['Quantity', 'UnitPrice', 'Revenue']]  # Select relevant numeric features for clustering

"""## **4.2 Standardize Numeric Features**"""

scaler = StandardScaler()
numeric_features_standardized = scaler.fit_transform(numeric_features) # Standardize numeric features

"""## **4.3 Perform Clustering**"""

num_clusters = 3
kmeans = KMeans(n_clusters=num_clusters, random_state=123)
non_uk_df['Cluster'] = kmeans.fit_predict(numeric_features_standardized)
clustered_non_uk_df = non_uk_df[['Quantity', 'UnitPrice', 'Revenue', 'Cluster', 'Country']]
clustered_non_uk_df.head()